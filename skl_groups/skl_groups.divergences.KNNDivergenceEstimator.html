

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>skl_groups.divergences.KNNDivergenceEstimator &mdash; skl-groups 0.1.0-dev documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="skl-groups 0.1.0-dev documentation" href="../index.html"/>
        <link rel="up" title="API reference" href="../reference.html"/>
        <link rel="next" title="skl_groups.kernels.PairwisePicker" href="skl_groups.kernels.PairwisePicker.html"/>
        <link rel="prev" title="skl_groups.summaries.L2DensityTransformer" href="skl_groups.summaries.L2DensityTransformer.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../index.html" class="fa fa-home"> skl-groups</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../who.html">Who this package is for</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#flann">FLANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#accelerated-version">Accelerated version</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Quick tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../reference.html#features-representation">Features representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference.html#preprocessing">Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference.html#summaries">Summaries</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../reference.html#divergences">Divergences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference.html#kernel-utilities">Kernel utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">Planned improvements</a></li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">skl-groups</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../reference.html">API reference</a> &raquo;</li>
      
    <li>skl_groups.divergences.KNNDivergenceEstimator</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="../_sources/skl_groups/skl_groups.divergences.KNNDivergenceEstimator.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="skl-groups-divergences-knndivergenceestimator">
<h1>skl_groups.divergences.KNNDivergenceEstimator<a class="headerlink" href="#skl-groups-divergences-knndivergenceestimator" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="skl_groups.divergences.KNNDivergenceEstimator">
<em class="property">class </em><tt class="descclassname">skl_groups.divergences.</tt><tt class="descname">KNNDivergenceEstimator</tt><big>(</big><em>div_funcs=('kl'</em>, <em>)</em>, <em>Ks=(3</em>, <em>)</em>, <em>do_sym=False</em>, <em>n_jobs=1</em>, <em>clamp=True</em>, <em>min_dist=0.001</em>, <em>flann_algorithm='auto'</em>, <em>flann_args=None</em>, <em>version='best'</em>, <em>memory=Memory(cachedir=None)</em><big>)</big><a class="headerlink" href="#skl_groups.divergences.KNNDivergenceEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates various divergence functions between bags.</p>
<p>Assumes that each bag represents an independent and identically distributed
sample from some unknown probability distribution (on which certain
technical assumptions are made), and estimates various distances between
them. The valid divergence functions are, where p and q refer to the density
functions for two bags:</p>
<ul class="simple">
<li>&#8216;kl&#8217;: The Kullback-Liebler divergence, which acts like a distance and has
some attractive information-theoretic properties as well as often giving
good results on machine learning problems in practice. Values are
nonnegative but can go to infinity.
<span class="math">\(\int p(x) \log\left( \frac{p(x)}{q(x)} \right) dx\)</span>.
Estimated as in <a class="reference internal" href="#r6" id="id1">[R6]</a>.</li>
<li>&#8216;renyi:.8&#8217; or any other number (other than 1): The Renyi-alpha divergence,
<span class="math">\(\frac{1}{\alpha - 1} \log \int p(x) \left( \frac{p(x)}{q(x)} \right)^{\alpha - 1} dx\)</span>.
Converges to &#8216;kl&#8217; as <span class="math">\(\alpha\)</span> goes to 1.
Values are nonnegative, but can go to infinity.
Estimated as in <a class="reference internal" href="#r7" id="id2">[R7]</a>.</li>
<li>&#8216;tsallis:.8&#8217; or any other number (other than 1): The Tsallis-alpha divergence,
<span class="math">\(\frac{1}{\alpha - 1} \left( \int p(x) \left(\frac{p(x)}{q(x)} \right)^{\alpha - 1} dx - 1 \right)\)</span>.
Converges to <cite>kl</cite> as <span class="math">\(\alpha\)</span> goes to 1.
Values are nonnegative, but can go to infinity.
Estimated as in <a class="reference internal" href="#r7" id="id3">[R7]</a>.</li>
<li>&#8216;hellinger&#8217;: The Hellinger distance, which is a true distance (i.e. it is
symmetric and its true value satisfies the triangle inequality) and is
between 0 and 1. Defined by <span class="math">\(\sqrt{1 - \int \sqrt{p(x) q(x)} dx}\)</span>.
Estimated as in <a class="reference internal" href="#r7" id="id4">[R7]</a>.</li>
<li>&#8216;bc&#8217;: The Bhattacharyya coefficient, <cite>:math:int sqrt{p(x) q(x)} dx</cite>.
This is an affinity rather than a distance and is between 0 and 1.
Estimated as in <a class="reference internal" href="#r7" id="id5">[R7]</a>.</li>
<li>&#8216;l2&#8217;: The <span class="math">\(L_2\)</span> distance between density functions
<span class="math">\(\sqrt{ \int (p(x) - q(x))^2 dx }\)</span>.
Estimated as in <a class="reference internal" href="#r7" id="id6">[R7]</a>.
A true distance function (symmetric and satisfies the triangle inequality.)</li>
<li>&#8216;linear&#8217;: <span class="math">\(\int p(x) q(x) dx\)</span>.
Estimated as in <a class="reference internal" href="#r7" id="id7">[R7]</a>.</li>
<li>&#8216;jensen-shannon&#8217; or &#8216;js&#8217;: The Jensen-Shannon divergence.
Equal to <span class="math">\(\frac{1}{2} D(p \| M) + \tfrac{1}{2} D(q \| M)\)</span>,
where D is the Kullback-Liebler divergence
and M is an equal mixture between p and q,
or equivalently <span class="math">\(H[M] - \frac{1}{2} H[p] - \frac{1}{2} H[q]\)</span>
where H is the Shannon entropy.
Symmetric, its square root satisfies the triangle inequality,
and is between 0 and <span class="math">\(\ln 2\)</span>.
Estimated using <a class="reference internal" href="#r8" id="id8">[R8]</a>.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>div_funcs</strong> : sequence of strings, optional, default [&#8216;kl&#8217;]</p>
<blockquote>
<div><p>A sequence of divergence function spec strings, as above.
For Renyi or Tsallis divergences, you can pass multiple values of
alpha, e.g. <tt class="docutils literal"><span class="pre">['renyi:.9',</span> <span class="pre">'renyi:.8',</span> <span class="pre">'tsallis:.99']</span></tt>.</p>
</div></blockquote>
<p><strong>Ks</strong> : sequence of positive integers, optional, default [3]</p>
<blockquote>
<div><p>The Ks to use for the K-nearest-neighbor estimator.
If you have very small bags (sizes less than, say, 25), try 1 or 2;
if large, say more than 1000, try 4 or 5.
Must be less than the smallest bag size; for the proof of <a class="reference internal" href="#r7" id="id9">[R7]</a> to work,
should be at least 3 for most divergence functions.
Jensen-Shannon treats this parameter a little differently.</p>
</div></blockquote>
<p><strong>do_sym</strong> : boolean, optional, default False</p>
<blockquote>
<div><p>As well as returning D(X || Y), return D(Y || X).</p>
</div></blockquote>
<p><strong>n_jobs</strong> : integer, optional, default 1</p>
<blockquote>
<div><p>The number of CPUs to use in the computation. -1 means &#8216;all CPUs&#8217;.</p>
</div></blockquote>
<p><strong>clamp</strong> : boolean, optional, default True</p>
<blockquote>
<div><p>For functions with bounded outputs, &#8220;clamp&#8221; them to lie within that
range; for example, if the estimator for KL divergence gives a negative
value, return 0 instead.</p>
</div></blockquote>
<p><strong>min_dist</strong> : float, optional, default 1e-3</p>
<blockquote>
<div><p>Protect against nearly-identical points by treating any distances less
than this amount as this number. Tiny distances screw up the estimators
because they assume that the inputs are from a continuous distribution,
where this doesn&#8217;t happen.</p>
</div></blockquote>
<p><strong>flann_algorithm</strong> : string, optional, default &#8216;auto&#8217;</p>
<blockquote>
<div><p>Which algorithm to use in FLANN for nearest neighbors. Defaults to
&#8216;auto&#8217;, which chooses &#8216;kdtree_single&#8217; if the input dimension is at most
5 and &#8216;linear&#8217; otherwise. In high-ish dimensional spaces, you can get
much better performance at the cost of approximate answers by using
other index types; see the FLANN documentation.</p>
</div></blockquote>
<p><strong>flann_args</strong> : dictionary, optional, default {}</p>
<blockquote>
<div><p>Other arguments to pass to FLANN.</p>
</div></blockquote>
<p><strong>version</strong> : one of {&#8216;best&#8217;, &#8216;fast&#8217;, &#8216;slow&#8217;}, optional, default &#8216;best&#8217;</p>
<blockquote>
<div><p>Whether to use the fast Cython implementation from skl-groups-accel
or the slower pure-Python implementation. &#8216;best&#8217; chooses &#8216;fast&#8217; if
available and you aren&#8217;t using custom divergence functions, &#8216;slow&#8217;
otherwise.</p>
</div></blockquote>
<p><strong>memory</strong> : Instance of <tt class="xref py py-class docutils literal"><span class="pre">joblib.Memory</span></tt> or string, optional</p>
<blockquote class="last">
<div><p>Used to cache the indices and the output of <a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.transform" title="skl_groups.divergences.KNNDivergenceEstimator.transform"><tt class="xref py py-meth docutils literal"><span class="pre">transform()</span></tt></a>.
By default, no caching is done. If a string is given, it is the
path to the caching directory.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The convergence proof in <a class="reference internal" href="#r6" id="id10">[R6]</a> is incorrect. The estimator seems to work
well, in practice, though.</p>
<p>The Jensen-Shannon estimation is performed by using the estimator of
<a class="reference internal" href="#r8" id="id11">[R8]</a> to get <span class="math">\(H[\frac{1}{2}(X + Y)]\)</span>, by combining the two samples
and assigning weights such that the total weight from each sample is equal,
and subtracting the mean of <span class="math">\(H[X]\)</span> and <span class="math">\(H[Y]\)</span> according to the
same estimator (with equally-weighted points). The <tt class="docutils literal"><span class="pre">K</span></tt> parameter is
used for the value of <span class="math">\(M = n \alpha\)</span> in <a class="reference internal" href="#r8" id="id12">[R8]</a>.</p>
<p class="rubric">References</p>
<table class="docutils citation" frame="void" id="r6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R6]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id10">2</a>, <a class="fn-backref" href="#id13">3</a>)</em> Q. Wang, S. Kulkarni, &amp; S. Verdu (2009).
Divergence Estimation for Multidimensional Densities Via
k-Nearest-Neighbor Distances.
IEEE Transactions on Information Theory, 55(5), 2392-2405.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="r7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R7]</td><td><em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id3">2</a>, <a class="fn-backref" href="#id4">3</a>, <a class="fn-backref" href="#id5">4</a>, <a class="fn-backref" href="#id6">5</a>, <a class="fn-backref" href="#id7">6</a>, <a class="fn-backref" href="#id9">7</a>, <a class="fn-backref" href="#id14">8</a>)</em> B. Poczos, L. Xiong, D. J. Sutherland, &amp; J. Schneider (2012).
Nonparametric kernel estimators for image classification.
In Computer Vision and Pattern Recognition (CVPR).</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="r8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[R8]</td><td><em>(<a class="fn-backref" href="#id8">1</a>, <a class="fn-backref" href="#id11">2</a>, <a class="fn-backref" href="#id12">3</a>, <a class="fn-backref" href="#id15">4</a>)</em> H. Hino &amp; N. Murata (2013).
Information estimators for weighted observations.
Neural Networks.</td></tr>
</tbody>
</table>
<p class="rubric">Attributes</p>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="98%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><cite>features_</cite></td>
<td>(<a class="reference internal" href="skl_groups.features.Features.html#skl_groups.features.Features" title="skl_groups.features.Features"><tt class="xref py py-class docutils literal"><span class="pre">skl_groups.features.Features</span></tt></a>) The features passed to <cite>fit</cite> (except made <tt class="xref py py-meth docutils literal"><span class="pre">sklearn.features.Features.bare()</span></tt>).</td>
</tr>
<tr class="row-even"><td><cite>indices_</cite></td>
<td>(list of <tt class="xref py py-class docutils literal"><span class="pre">cyflann.FLANNIndex</span></tt> or <tt class="xref py py-class docutils literal"><span class="pre">pyflann.FLANN</span></tt>) A FLANN index for each bag in <cite>features_</cite>.</td>
</tr>
<tr class="row-odd"><td><cite>rhos_</cite></td>
<td>(list of arrays) For each bag in <cite>features_</cite>, the distance to the Kth nearest neighbor of each point amongst its own bag. <tt class="docutils literal"><span class="pre">rhos_[i][j,</span> <span class="pre">k]</span></tt> is the distance to either the <tt class="docutils literal"><span class="pre">Ks[k]</span></tt> th or the <tt class="docutils literal"><span class="pre">k+1</span></tt> th nearest neighbor of <tt class="docutils literal"><span class="pre">features_[i][j,</span> <span class="pre">:]</span></tt> in <tt class="docutils literal"><span class="pre">features_[i]</span></tt> (not including <tt class="docutils literal"><span class="pre">features_[i][j,</span> <span class="pre">:]</span></tt> itself). It&#8217;s the <tt class="docutils literal"><span class="pre">k+1</span></tt> th if Jensen-Shannon divergence is requested.  May or may not be present after <a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.fit" title="skl_groups.divergences.KNNDivergenceEstimator.fit"><tt class="xref py py-meth docutils literal"><span class="pre">fit()</span></tt></a>; will be after <a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.transform" title="skl_groups.divergences.KNNDivergenceEstimator.transform"><tt class="xref py py-meth docutils literal"><span class="pre">transform()</span></tt></a>.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="skl_groups.divergences.KNNDivergenceEstimator.__init__">
<tt class="descname">__init__</tt><big>(</big><em>div_funcs=('kl'</em>, <em>)</em>, <em>Ks=(3</em>, <em>)</em>, <em>do_sym=False</em>, <em>n_jobs=1</em>, <em>clamp=True</em>, <em>min_dist=0.001</em>, <em>flann_algorithm='auto'</em>, <em>flann_args=None</em>, <em>version='best'</em>, <em>memory=Memory(cachedir=None)</em><big>)</big><a class="headerlink" href="#skl_groups.divergences.KNNDivergenceEstimator.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p class="rubric">Methods</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.__init__" title="skl_groups.divergences.KNNDivergenceEstimator.__init__"><tt class="xref py py-obj docutils literal"><span class="pre">__init__</span></tt></a>([div_funcs,&nbsp;Ks,&nbsp;do_sym,&nbsp;n_jobs,&nbsp;...])</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.fit" title="skl_groups.divergences.KNNDivergenceEstimator.fit"><tt class="xref py py-obj docutils literal"><span class="pre">fit</span></tt></a>(X[,&nbsp;y,&nbsp;get_rhos])</td>
<td>Sets up for divergence estimation &#8220;from&#8221; new data &#8220;to&#8221; X.</td>
</tr>
<tr class="row-odd"><td><tt class="xref py py-obj docutils literal"><span class="pre">fit_transform</span></tt>(X[,&nbsp;y])</td>
<td>Fit to data, then transform it.</td>
</tr>
<tr class="row-even"><td><tt class="xref py py-obj docutils literal"><span class="pre">get_params</span></tt>([deep])</td>
<td>Get parameters for this estimator.</td>
</tr>
<tr class="row-odd"><td><tt class="xref py py-obj docutils literal"><span class="pre">set_params</span></tt>(**params)</td>
<td>Set the parameters of this estimator.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.transform" title="skl_groups.divergences.KNNDivergenceEstimator.transform"><tt class="xref py py-obj docutils literal"><span class="pre">transform</span></tt></a>(X)</td>
<td>Computes the divergences from X to <tt class="xref py py-attr docutils literal"><span class="pre">features_</span></tt>.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="skl_groups.divergences.KNNDivergenceEstimator.fit">
<tt class="descname">fit</tt><big>(</big><em>X</em>, <em>y=None</em>, <em>get_rhos=False</em><big>)</big><a class="headerlink" href="#skl_groups.divergences.KNNDivergenceEstimator.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets up for divergence estimation &#8220;from&#8221; new data &#8220;to&#8221; X.
Builds FLANN indices for each bag, and maybe gets within-bag distances.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>X</strong> : list of arrays or <a class="reference internal" href="skl_groups.features.Features.html#skl_groups.features.Features" title="skl_groups.features.Features"><tt class="xref py py-class docutils literal"><span class="pre">skl_groups.features.Features</span></tt></a></p>
<blockquote>
<div><p>The bags to search &#8220;from&#8221;.</p>
</div></blockquote>
<p><strong>get_rhos</strong> : boolean, optional, default False</p>
<blockquote class="last">
<div><p>Compute within-bag distances <tt class="xref py py-attr docutils literal"><span class="pre">rhos_</span></tt>. These are only needed
for some divergence functions or if do_sym is passed, and they&#8217;ll
be computed (and saved) during <a class="reference internal" href="#skl_groups.divergences.KNNDivergenceEstimator.transform" title="skl_groups.divergences.KNNDivergenceEstimator.transform"><tt class="xref py py-meth docutils literal"><span class="pre">transform()</span></tt></a> if they&#8217;re not
computed here.</p>
<p>If you&#8217;re using Jensen-Shannon divergence, a higher max_K may
be needed once it sees the number of points in the transformed bags,
so the computation here might be wasted.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="skl_groups.divergences.KNNDivergenceEstimator.transform">
<tt class="descname">transform</tt><big>(</big><em>X</em><big>)</big><a class="headerlink" href="#skl_groups.divergences.KNNDivergenceEstimator.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the divergences from X to <tt class="xref py py-attr docutils literal"><span class="pre">features_</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>X</strong> : list of bag feature arrays or <a class="reference internal" href="skl_groups.features.Features.html#skl_groups.features.Features" title="skl_groups.features.Features"><tt class="xref py py-class docutils literal"><span class="pre">skl_groups.features.Features</span></tt></a></p>
<blockquote>
<div><p>The bags to search &#8220;from&#8221;.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>divs</strong> : array of shape <tt class="docutils literal"><span class="pre">[len(div_funcs),</span> <span class="pre">len(Ks),</span> <span class="pre">len(X),</span> <span class="pre">len(features_)]</span> <span class="pre">+</span> <span class="pre">([2]</span> <span class="pre">if</span> <span class="pre">do_sym</span> <span class="pre">else</span> <span class="pre">[])</span></tt></p>
<blockquote class="last">
<div><p>The divergences from X to <tt class="xref py py-attr docutils literal"><span class="pre">features_</span></tt>.
<tt class="docutils literal"><span class="pre">divs[d,</span> <span class="pre">k,</span> <span class="pre">i,</span> <span class="pre">j]</span></tt> is the <tt class="docutils literal"><span class="pre">div_funcs[d]</span></tt> divergence
from <tt class="docutils literal"><span class="pre">X[i]</span></tt> to <tt class="docutils literal"><span class="pre">fetaures_[j]</span></tt> using a K of <tt class="docutils literal"><span class="pre">Ks[k]</span></tt>.
If <tt class="docutils literal"><span class="pre">do_sym</span></tt>, <tt class="docutils literal"><span class="pre">divs[d,</span> <span class="pre">k,</span> <span class="pre">i,</span> <span class="pre">j,</span> <span class="pre">0]</span></tt> is
<span class="math">\(D_{d,k}( X_i \| \texttt{features_}_j)\)</span> and
<tt class="docutils literal"><span class="pre">divs[d,</span> <span class="pre">k,</span> <span class="pre">i,</span> <span class="pre">j,</span> <span class="pre">1]</span></tt> is <span class="math">\(D_{d,k}(\texttt{features_}_j \| X_i)\)</span>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="skl_groups.kernels.PairwisePicker.html" class="btn btn-neutral float-right" title="skl_groups.kernels.PairwisePicker"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="skl_groups.summaries.L2DensityTransformer.html" class="btn btn-neutral" title="skl_groups.summaries.L2DensityTransformer"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Dougal J. Sutherland.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.1.0-dev',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>